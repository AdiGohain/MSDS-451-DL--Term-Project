{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c855c1-9c2d-4301-86a0-f245c39615c8",
   "metadata": {},
   "source": [
    "# Adi Gohain \n",
    "# Term Project \n",
    "# MSDS Financial Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df3892f-3fb1-4da4-8870-aac969c8edde",
   "metadata": {},
   "source": [
    "### LOADING EXTERNAL LIBRARIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08306603-ccb4-4a35-b9aa-a55ed9af30ae",
   "metadata": {},
   "source": [
    "In the section below, I load all external libraries required for the trading system.\n",
    "\n",
    "- **NumPy**: vectorized numerical operations, especially for portfolio simulations.\n",
    "- **Pandas**: time series handling, financial data structures, percent-change returns, rolling windows, etc.\n",
    "- **Matplotlib & Seaborn**: visualization of strategy performance, risk metrics, and Monte Carlo results.\n",
    "- **yfinance**: real-time historical price downloads for stocks, ETFs, and futures.\n",
    "- **os & datetime**: directory management and date formatting for saved outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e99c9b3-2993-44d2-82c9-18841737969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154079c1-4f9d-4fb5-8d05-6d9b6d964c3b",
   "metadata": {},
   "source": [
    "### INGESTION OF DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da157a-635d-4b10-b9e1-b0734daf5682",
   "metadata": {},
   "source": [
    "Here I am defining the global tickers for my commodities, equities, and benchmarks.\n",
    "I did this in order to make make the system modular — changing tickers requires editing only these blocks.\n",
    "I also decided to create directories OUTPUT_DIR and CURVES_DIR to create a clean file structure for project output, figures, and logs. This project can be run easily from any root folder and the code will automatically save project files by creating these new folders.\n",
    "\n",
    "I am also including the following ETF's in my mix to add diversification. Adding these uncorrelated ETF's with the four industry assets allows me to improve portfolio performance. \n",
    "\n",
    "Here is the reasoning behind picking these ETF's: (These ETF's were suggested by Chat GPT)\n",
    "\n",
    "Asset Classes With Low or Negative Correlation to Industrials\n",
    "Long-Duration U.S. Treasuries (defensive macro hedge)\n",
    "- 20+ Year U.S. Treasuries **TLT**\n",
    "- 7–10 Year Treasuries **IEF**\n",
    "  \n",
    "Commodities and Real Assets (Energy & Metals/ Non Agricultural) \n",
    "- Gold **GLD**\n",
    "- Broad Commodities (energy-weighted) **DBC**\n",
    "\n",
    "International Diversification / Developed Markets (non-U.S.)\n",
    "- Developed Markets ex-US **EFA**\n",
    "- Vanguard Developed ex-US **VEA**\n",
    "\n",
    "Market-Neutral / Low-Beta / Risk-Managed Strategies\n",
    "- Anti-Beta (long low beta, short high beta) **BTAL** \n",
    "- Multi-strategy, alt-risk premia **LALT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769b6019-5013-446a-9eef-2344434db057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am using 4 common agricultural commodities corn, soybeans, cotton and wheat\n",
    "COMMODITY_TICKERS = {\"Corn\": \"ZC=F\",\n",
    "                     \"Soybeans\": \"ZS=F\",\n",
    "                     \"Cotton\": \"CT=F\",\n",
    "                     \"Wheat\": \"ZW=F\"}\n",
    "\n",
    "# Here I am using 4 equities: John Deere, Caterpillar, AgCo Corporation and CNH Industrial\n",
    "\n",
    "EQUITY_TICKERS = {\"DE\": \"DE\",\n",
    "                  \"CAT\": \"CAT\",\n",
    "                  \"AGCO\": \"AGCO\",\n",
    "                  \"CNH\": \"CNH\",\n",
    "                  \"20+ Year U.S. Treasuries\":\"TLT\", \n",
    "                  \"7–10 Year Treasuries\":\"IEF\",\n",
    "                  \"Gold\":\"GLD\",\n",
    "                  \"Broad Commodities (energy-weighted)\":\"DBC\",\n",
    "                  \"Developed Markets ex-US\":\"EFA\",\n",
    "                  \"Vanguard Developed ex-US\":\"VEA\",\n",
    "                  \"Anti-Beta (long low beta, short high beta)\":\"BTAL\", \n",
    "                  \"Multi-strategy, alt-risk premia\":\"LALT\"}\n",
    "\n",
    "BENCHMARK_TICKERS = {\"SPY\": \"SPY\",\n",
    "                     \"QQQ\": \"QQQ\",\n",
    "                     \"BND\": \"BND\"}\n",
    "\n",
    "# Assigning historical data start date to a variable START so it can be changed anytime and cascades downstream.\n",
    "START = \"2000-01-01\"\n",
    "\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "CURVES_DIR = os.path.join(OUTPUT_DIR, \"curves\")\n",
    "\n",
    "os.makedirs(CURVES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee12732-f4c3-4d7e-ac6e-ab4898e43a6c",
   "metadata": {},
   "source": [
    "Below, the download_prices function first downloads historical price data from Yahoo Finance for the group of tickers assigned above.\n",
    "I was running into various issues where tickers had varying timelines as well as dataframes had format errors and therefore to mitigate this, I added logic to handles multi index futures data from Yahoo (ex. \"Adj Close\" nested under the ticker symbol).\n",
    "\n",
    "I used Chat GPT to develop a logic to resolve:\n",
    "- Skipping unavailable tickers instead of crashing the pipeline.\n",
    "- Using adjusted close prices, which reflect corporate actions (splits, dividends).\n",
    "- Ensuring all time indices are converted to DatetimeIndex, avoiding alignment bugs later.\n",
    "- Collecting all valid series into a dictionary and concatenating them horizontally, to create a clean, consistent price matrix where each column corresponds to a single asset.\n",
    "- Raising an error to prevent any failures if no data is successfully retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc50131-c3e3-4503-b534-eb4b508c50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_prices(ticker_dict, start=START):\n",
    "    prices = {}\n",
    "    for name, ticker in ticker_dict.items():\n",
    "        print(f\"Downloading {name} ({ticker})...\")\n",
    "        try:\n",
    "            data = yf.download(ticker, start=start, auto_adjust=False, progress=False)\n",
    "            if data is None or data.empty:\n",
    "                print(f\"Warning: no data available for {ticker}. Therefore, skipping {ticker}.\")\n",
    "                continue\n",
    "\n",
    "            # If multi index futures, then select ('Adj Close', ticker)\n",
    "            if isinstance(data.columns, pd.MultiIndex):\n",
    "                try:\n",
    "                    series = data[\"Adj Close\"][ticker].dropna()\n",
    "                except Exception:\n",
    "                    print(f\"Warning: Multi index but can not extract Adjusted Close for {ticker}. Therefore, skipping {ticker}.\")\n",
    "                    continue\n",
    "            else:\n",
    "                if \"Adj Close\" not in data.columns:\n",
    "                    print(f\"Warning: 'Adj Close' is missing for {ticker}. Therefore, skipping {ticker}.\")\n",
    "                    continue\n",
    "                series = data[\"Adj Close\"].dropna()\n",
    "\n",
    "            if series.empty:\n",
    "                print(f\"Warning: Adjusted close is empty for {ticker}. Therefore, skipping {ticker}.\")\n",
    "                continue\n",
    "\n",
    "            # ensuring datetime index\n",
    "            series.index = pd.to_datetime(series.index)\n",
    "            prices[name] = series\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error downloading {ticker}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(prices) == 0:\n",
    "        raise ValueError(\"No valid tickers downloaded.\")\n",
    "\n",
    "    df = pd.concat(prices, axis=1).sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455afbb-4865-42d4-a4e2-196660e6c179",
   "metadata": {},
   "source": [
    "### ALIGNING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac9af4-4302-4290-b8bc-9480a2b5c8a3",
   "metadata": {},
   "source": [
    "Because financial assets had different trading calendars like holidays, futures roll dates, missing history, etc..\n",
    "\n",
    "I incorporated this function to synchronize the commodity futures, equity time series, and my benchmark ETFs.\n",
    "\n",
    "This made sure that all datasets shared a common calendar using index intersection.\n",
    "Forward filling helped in handling short gaps like holidays or missing futures data.\n",
    "Rows with all missing data are being dropped to preserve numerical integrity.\n",
    "\n",
    "This was an important process to add because without it, the vectorized operations (weights × returns) were getting misaligned and producing incorrect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07014c9a-4241-4aa5-9cb3-709513ba926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_data(comm, eq, benchmarks=None):\n",
    "    # Using intersection to ensure exact alignment\n",
    "    if benchmarks is None:\n",
    "        shared = comm.index.intersection(eq.index)\n",
    "    else:\n",
    "        shared = comm.index.intersection(eq.index).intersection(benchmarks.index)\n",
    "        comm_a = comm.reindex(shared).ffill().dropna(how='all')\n",
    "        eq_a = eq.reindex(shared).ffill().dropna(how='all')\n",
    "    \n",
    "    if benchmarks is not None:\n",
    "        bench_a = benchmarks.reindex(shared).ffill().dropna(how='all')\n",
    "    else:\n",
    "        bench_a = None\n",
    "        shared = comm_a.index.intersection(eq_a.index)\n",
    "    \n",
    "    if bench_a is not None:\n",
    "        shared = shared.intersection(bench_a.index)\n",
    "        comm_a = comm_a.loc[shared]\n",
    "        eq_a = eq_a.loc[shared]\n",
    "    \n",
    "    if bench_a is not None:\n",
    "        bench_a = bench_a.loc[shared]\n",
    "\n",
    "    print(f\"Aligned rows: {len(shared)} ({shared[0].date()} to {shared[-1].date()})\")\n",
    "    return comm_a, eq_a, bench_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7794918-2b84-4d01-bafd-b35f1851a6ca",
   "metadata": {},
   "source": [
    "### SIGNAL GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e9d39-bc70-4a07-920a-8eb5ad83f4bd",
   "metadata": {},
   "source": [
    "In my model, I use momentum signals from commodities.\n",
    "\n",
    "A commodity is “bullish” when its price closes above its moving average.\n",
    "I am combining multiple commodities to create a broad commodity risk-on signal.\n",
    "I am doing so by incorporating two functions here:\n",
    "1. **compute_ma_signal()**\n",
    "\n",
    "This one computes a binary indicator:\n",
    "1 if price is above its moving average\n",
    "0 is price is below\n",
    "This represents a short term commodity momentum.\n",
    "\n",
    "2. **aggregate_signal()**\n",
    "\n",
    "This one combines all commodities by summing signals:\n",
    "If at least N commodities are bullish, the model enters equities.\n",
    "By doing so I am reducing noise and avoiding overreacting to single commodity volatility.\n",
    "\n",
    "Because commodities are often leading indicators for industrial/equipment sector performance.\n",
    "I chose to use this rule to captures macroeconomic demand conditions without relying on equity prices directly.\n",
    "\n",
    "3. **apply_persistence_and_hold()**\n",
    "\n",
    "I added this function to implement realistic trading constraints to my raw signals:\n",
    "*min_consec_days* requires consecutive bullish days before entering a position. This is done in order to filters out whipsaw signals.\n",
    "*min_hold_days* enforces a mandatory holding period once a position is entered therefore preventing overtrading and reducing transaction costs. This mimics institutional trading mandates where churn is expensive.\n",
    "Internally, a finite-state loop steps day-by-day to extend a buy signal across required holding windows.\n",
    "This produces a more stable and realistic signal series for backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e933298-fc98-4939-8464-1373ae301cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ma_signal(price, window):\n",
    "    ma = price.rolling(window, min_periods=1).mean()\n",
    "    return (price > ma).astype(int)\n",
    "\n",
    "def aggregate_signal(comm_df, ma_window=20, threshold=2):\n",
    "    sigs = {}\n",
    "    for col in comm_df.columns:\n",
    "        sigs[col] = compute_ma_signal(comm_df[col], ma_window)\n",
    "    sig_df = pd.DataFrame(sigs)\n",
    "    sig_df['sum'] = sig_df.sum(axis=1)\n",
    "    sig_df['raw_signal'] = (sig_df['sum'] >= threshold).astype(int)\n",
    "    return sig_df\n",
    "\n",
    "def apply_persistence_and_hold(raw_signal, min_consec_days=0, min_hold_days=0):\n",
    "    s = raw_signal.astype(int).copy()\n",
    "    if min_consec_days > 0:\n",
    "        rsum = s.rolling(min_consec_days).sum()\n",
    "        s = (rsum >= min_consec_days).astype(int)\n",
    "    if min_hold_days > 0:\n",
    "        out = pd.Series(0, index=s.index)\n",
    "        i = 0\n",
    "        idx = list(s.index)\n",
    "        n = len(idx)\n",
    "        while i < n:\n",
    "            if s.iloc[i] == 1:\n",
    "                hold_end = min(i + min_hold_days, n)\n",
    "                j = i\n",
    "                while j < n and (j < hold_end or s.iloc[j] == 1):\n",
    "                    out.iloc[j] = 1\n",
    "                    j += 1\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        return out\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450bb35c-6a91-4b41-8b30-f538dcb36291",
   "metadata": {},
   "source": [
    "### BACKTESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbcb061-4312-468a-88c7-817f0d0c9e6e",
   "metadata": {},
   "source": [
    "Below, the backtesting process is my core module where I am translates my trading signals into daily portfolio returns.\n",
    "\n",
    "I designed it to performs the following steps:\n",
    "1. First computing the daily equity returns using percent change.\n",
    "2. Then Lagging the signal by a day for preventing lookahead bias.\n",
    "3. When signal equals 1, the algorithm invest equally across all equities\n",
    "4. When signal = 0, the algorithm holds 0% exposure.\n",
    "\n",
    "For estimating transaction costs realistically, I am calculating turnover by measuring how much the portfolio changes day to day.\n",
    "\n",
    "Transaction cost model being applied daily to produce net returns.\n",
    "- Costs = turnover × (commission + slippage)\n",
    "\n",
    "Gross and net cumulative equity curves are being calculated via cumulative products.\n",
    "\n",
    "The output df then includes:\n",
    "gross_ret\n",
    "net_ret\n",
    "turnover\n",
    "pos (exposure)\n",
    "gross_eq\n",
    "net_eq\n",
    "\n",
    "I am doing this primarily to prepares my results for easy plotting, analyzing, and exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c577a5-cfbc-42f7-9b4b-5a8ebce5e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_strategy_returns(eq_prices, signal, commission=0.0005, slippage=0.0005):\n",
    "    eq_ret = eq_prices.pct_change().fillna(0)\n",
    "    pos = signal.shift(1).fillna(0)  # lagged signal\n",
    "    n_assets = eq_prices.shape[1]\n",
    "    # ensuring positive index matches eq_prices\n",
    "    pos = pos.reindex(eq_prices.index).fillna(0)\n",
    "    pos_df = pd.DataFrame(np.outer(pos, np.ones(n_assets)),\n",
    "                          index=eq_prices.index,\n",
    "                          columns=eq_prices.columns)\n",
    "    \n",
    "    weights = pos_df / n_assets\n",
    "    \n",
    "    prev_weights = weights.shift(1).fillna(0)\n",
    "    \n",
    "    turnover = (weights - prev_weights).abs().sum(axis=1) / 2.0\n",
    "    \n",
    "    cost = turnover * (commission + slippage)\n",
    "    \n",
    "    gross_ret = (weights * eq_ret).sum(axis=1)\n",
    "    \n",
    "    net_ret = gross_ret - cost\n",
    "    \n",
    "    df = pd.DataFrame({\"gross_ret\": gross_ret,\n",
    "                       \"net_ret\": net_ret,\n",
    "                       \"turnover\": turnover,\n",
    "                       \"pos\": pos})\n",
    "    \n",
    "    df[\"gross_eq\"] = (1 + df[\"gross_ret\"]).cumprod()\n",
    "    df[\"net_eq\"] = (1 + df[\"net_ret\"]).cumprod()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96256fc5-b874-4d32-ab34-7b1d7c87c1b8",
   "metadata": {},
   "source": [
    "### EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a4dc5-da35-4332-a8d9-1cf4462ebeb7",
   "metadata": {},
   "source": [
    "Here I am summarizing the strategy performance into key evaluation metrics:\n",
    "1. Annualized return: compounded daily net returns.\n",
    "2. Annualized volatility: daily standard deviation × √252.\n",
    "3. Sharpe ratios: risk-adjusted performance indicator.\n",
    "4. Average turnover: measure of trading intensity.\n",
    "5. Total trades: days when turnover > 0.\n",
    "6. Days in market: proxy for exposure and signal frequency.\n",
    "7. Final net equity: ending value of $1 invested.\n",
    "\n",
    "Using these metrics, I am comparing strategy variants, MA windows, or parameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ad3251-5456-471f-bd48-077385ed6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(res):\n",
    "    \n",
    "    mean = res[\"net_ret\"].mean()\n",
    "   \n",
    "    vol = res[\"net_ret\"].std()\n",
    "    \n",
    "    # Annualized return\n",
    "    ann_ret = (1 + mean) ** 252 - 1\n",
    "    # Annualized volatility\n",
    "    ann_vol = vol * np.sqrt(252)\n",
    "    # Sharpe Ratios\n",
    "    sharpe = (mean / vol * np.sqrt(252)) if vol > 0 else np.nan\n",
    "    \n",
    "    return {\"Ann_Return\": ann_ret,\n",
    "            \"Ann_Vol\": ann_vol,\n",
    "            \"Sharpe\": sharpe,\n",
    "            \"Avg_Turnover\": res[\"turnover\"].mean(),\n",
    "            \"Total_Trades\": (res[\"turnover\"] > 0).sum(),\n",
    "            \"Days_In_Market\": res[\"pos\"].sum(),\n",
    "            \"Final_Net_Equity\": res[\"net_eq\"].iloc[-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf11c0f-6b80-45bc-a2fb-5458d6524da6",
   "metadata": {},
   "source": [
    "### VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39586ab-5b5d-4721-a66b-6ecf8037a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equity Curve Plot: To show gross vs. net cumulative returns over time.\n",
    "def plot_strategy_equity(res, name=\"strategy_equity.png\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(res[\"gross_eq\"], label=\"Gross\")\n",
    "    plt.plot(res[\"net_eq\"], label=\"Net\")\n",
    "    plt.title(\"Strategy Equity Curve\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Cumulative Return\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CURVES_DIR, name)); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12211f84-08ba-45b7-baf4-c512f35c524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Comparison Plot: To shows strategy performance against SPY/QQQ/BND.\n",
    "def plot_equity_vs_benchmarks(res, benchmarks_df, name=\"eq_vs_benchmarks.png\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(res[\"net_eq\"], label=\"Strategy (Net)\")\n",
    "    # building benchmark cum returns from adj close\n",
    "    for col in benchmarks_df.columns:\n",
    "        bret = benchmarks_df[col].pct_change().fillna(0)\n",
    "        bcum = (1 + bret).cumprod()\n",
    "        plt.plot(bcum, label=col)\n",
    "    plt.title(\"Strategy vs Benchmarks\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Cumulative Return\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CURVES_DIR, name)); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9fe9278-2222-4acd-a22b-db38c9fee848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Exposure: To show how often the strategy is being invested.. for interpreting Sharpe and turnover.\n",
    "def plot_cumulative_exposure(res, name=\"cumulative_exposure.png\"):\n",
    "    cum_exposure = res[\"pos\"].cumsum()\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(cum_exposure, label=\"Cumulative Days in Market\")\n",
    "    plt.title(\"Cumulative Exposure (Days in Market)\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Days\"); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CURVES_DIR, name)); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ba5bdf3-26ce-4e1a-b169-f2dd2346221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Returns & Distribution: Shows cumulative returns, and also plots histogram of daily returns with KDE. I wanted \n",
    "# To do this as it was useful for understanding risk, skewness, and tail behavior.\n",
    "def plot_daily_returns_and_hist(res, name_ts=\"daily_returns_ts.png\", name_hist=\"daily_returns_hist.png\"):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(res[\"net_ret\"].cumsum(), label=\"Cumulative Net Returns (for reference)\")\n",
    "    plt.title(\"Cumulative Sum of Daily Net Returns\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CURVES_DIR, name_ts)); plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(res[\"net_ret\"].dropna(), bins=80, kde=True)\n",
    "    plt.title(\"Histogram of Daily Net Returns\")\n",
    "    plt.xlabel(\"Daily Net Return\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CURVES_DIR, name_hist)); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fefb0db9-165b-4b70-b0ef-6e30106b695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commodity Moving Average Charts: Each commodity is being plotted with its moving average windows, showing signal rationale.\n",
    "def plot_commodity_ma(comm_df, ma_windows=[20,40], name_prefix=\"commodity_ma\"):\n",
    "    for col in comm_df.columns:\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.plot(comm_df[col], label=f\"{col} Price\")\n",
    "        for w in ma_windows:\n",
    "            ma = comm_df[col].rolling(w).mean()\n",
    "            plt.plot(ma, label=f\"{w}-day MA\")\n",
    "        plt.title(f\"{col} Price and Moving Averages\")\n",
    "        plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CURVES_DIR, f\"{name_prefix}_{col}.png\")); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b11beeb0-55a6-4659-85b6-a025c213cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap: Shows correlations between equities to check for diversification.\n",
    "def plot_correlation_heatmap(eq_prices, name=\"correlation_heatmap.png\"):\n",
    "    rets = eq_prices.pct_change().dropna()\n",
    "    corr = rets.corr()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(\"Equity Correlation Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CURVES_DIR, name)); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4d4be3f-28c4-4892-9863-b0318886eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Volatility: To measures risk changes over time.\n",
    "def plot_rolling_volatility(eq_prices, window=63, name=\"rolling_vol.png\"):\n",
    "    rets = eq_prices.pct_change().dropna()\n",
    "    port = rets.mean(axis=1)  # simple average portfolio for illustration\n",
    "    roll_vol = port.rolling(window).std() * np.sqrt(252)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(roll_vol)\n",
    "    plt.title(f\"Rolling Annualized Volatility (window={window})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CURVES_DIR, name)); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7dc875-3368-4979-ba45-ca02d61bf1bc",
   "metadata": {},
   "source": [
    "### MONTE CARLO SIMULATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34192acc-45fb-47f1-91db-66b49974adac",
   "metadata": {},
   "source": [
    "I optimized the Programming_Assignment_02 using chat GPT to bring in the Monte Carlo simulation as a module in this project. \n",
    "I am running 5000 random portfolio simulations using annualized mean returns and annualized covariance matrix\n",
    "\n",
    "Similar to the assignment, I am evaluating 2 cases:\n",
    "1. Long-only portfolios\n",
    "2. Long + short allowed (market-neutral) portfolios\n",
    "\n",
    "For each simulated weight vector, I am computing and storing the expected annual return, expected volatility, and Sharpe ratios\n",
    "\n",
    "I am also adding a scatter plot to visualize the comparison between long-only vs long–short allowed sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b76bbf4-84e3-4e86-8fdc-0b6ae3107bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_portfolios(returns_df, n_portfolios=5000, long_only=True, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    rets = returns_df.dropna().pct_change().dropna()\n",
    "    \n",
    "    mu = rets.mean() * 252\n",
    "    sigma = rets.cov() * 252\n",
    "    \n",
    "    results = []\n",
    "    for i in range(n_portfolios):\n",
    "        if long_only:\n",
    "            w = np.random.random(len(rets.columns))\n",
    "            w /= w.sum()\n",
    "        else:\n",
    "            w = np.random.normal(0,1,len(rets.columns))\n",
    "            # scale to have sum zero (allow leverage)\n",
    "            if np.sum(np.abs(w)) > 0:\n",
    "                w = w / np.sum(np.abs(w))\n",
    "        port_ret = np.dot(w, mu)\n",
    "        port_var = np.dot(w.T, np.dot(sigma, w))\n",
    "        port_vol = np.sqrt(port_var)\n",
    "        results.append({\n",
    "            \"ret\": port_ret,\n",
    "            \"vol\": port_vol,\n",
    "            \"sharpe\": (port_ret / port_vol if port_vol>0 else np.nan),\n",
    "            \"weights\": w\n",
    "        })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da08dc37-003d-47b2-afd9-faa82d671ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monte_carlo_scatter(mc_df_long, mc_df_short, name=\"monte_carlo.png\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(mc_df_long['vol'], mc_df_long['ret'], c=mc_df_long['sharpe'], cmap='viridis', s=8, label='Long-only')\n",
    "    plt.scatter(mc_df_short['vol'], mc_df_short['ret'], c=mc_df_short['sharpe'], cmap='plasma', s=8, marker='x', label='Long+Short')\n",
    "    plt.colorbar(label='Sharpe')\n",
    "    plt.xlabel(\"Annualized Volatility\")\n",
    "    plt.ylabel(\"Annualized Return\")\n",
    "    plt.title(\"Monte Carlo Portfolio Simulation (return vs volatility)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CURVES_DIR, name)); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d37c8-4216-4028-94e1-ea2591607e47",
   "metadata": {},
   "source": [
    "### RUNNING PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f778ca-b59b-4881-863d-56b20f7706d5",
   "metadata": {},
   "source": [
    "In the cell below, I am calling all the functions I created above to run my pipeline and then I am also outputting my data into a results df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a95b9a6b-4e69-4796-9689-685063f3ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline(start=START,\n",
    "                      ma_windows=[20,40],\n",
    "                      threshold=2,\n",
    "                      min_consec_days=0,\n",
    "                      min_hold_days=0,\n",
    "                      commission=0.0005,\n",
    "                      slippage=0.0005,\n",
    "                      n_mc=4000):\n",
    "    # downloading\n",
    "    comm = download_prices(COMMODITY_TICKERS, start=start)\n",
    "    eq = download_prices(EQUITY_TICKERS, start=start)\n",
    "    benches = download_prices(BENCHMARK_TICKERS, start=start)\n",
    "\n",
    "    # aligning\n",
    "    comm_a, eq_a, benches_a = align_data(comm, eq, benches)\n",
    "\n",
    "    # signal using primary MA (20) and alt MA (40) if needed.\n",
    "    # Here we produce both and run the pipeline with each MA.\n",
    "    results_summary = []\n",
    "\n",
    "    for ma in ma_windows:\n",
    "        sig_df = aggregate_signal(comm_a, ma_window=ma, threshold=threshold)\n",
    "        adj_sig = apply_persistence_and_hold(sig_df['raw_signal'],\n",
    "                                             min_consec_days=min_consec_days,\n",
    "                                             min_hold_days=min_hold_days)\n",
    "\n",
    "        # make sure adj_sig index matches eq_a\n",
    "        adj_sig = adj_sig.reindex(eq_a.index).fillna(0)\n",
    "\n",
    "        res = compute_strategy_returns(eq_a, adj_sig, commission=commission, slippage=slippage)\n",
    "        metrics = compute_metrics(res)\n",
    "        metrics.update({\n",
    "            \"MA\": ma,\n",
    "            \"MinConsec\": min_consec_days,\n",
    "            \"MinHold\": min_hold_days,\n",
    "            \"Commission\": commission,\n",
    "            \"Slippage\": slippage\n",
    "        })\n",
    "        results_summary.append(metrics)\n",
    "\n",
    "        # Saving daily results\n",
    "        fname = f\"results_ma{ma}_c{min_consec_days}_h{min_hold_days}.csv\"\n",
    "        res.to_csv(os.path.join(OUTPUT_DIR, fname))\n",
    "\n",
    "        # Ploting\n",
    "        plot_strategy_equity(res, name=f\"strategy_equity_ma{ma}.png\")\n",
    "        if benches_a is not None:\n",
    "            plot_equity_vs_benchmarks(res, benches_a, name=f\"eq_vs_benchmarks_ma{ma}.png\")\n",
    "        plot_cumulative_exposure(res, name=f\"cumulative_exposure_ma{ma}.png\")\n",
    "        plot_daily_returns_and_hist(res,\n",
    "                                    name_ts=f\"daily_returns_cumsum_ma{ma}.png\",\n",
    "                                    name_hist=f\"daily_returns_hist_ma{ma}.png\")\n",
    "        plot_commodity_ma(comm_a, ma_windows=[ma], name_prefix=f\"commodity_ma{ma}\")\n",
    "        plot_correlation_heatmap(eq_a, name=f\"corr_heatmap_ma{ma}.png\")\n",
    "        plot_rolling_volatility(eq_a, window=63, name=f\"rolling_vol_ma{ma}.png\")\n",
    "\n",
    "    # Running Monte Carlo on equities\n",
    "    mc_long = monte_carlo_portfolios(eq_a, n_portfolios=n_mc, long_only=True)\n",
    "    mc_short = monte_carlo_portfolios(eq_a, n_portfolios=n_mc, long_only=False)\n",
    "    mc_long.to_csv(os.path.join(OUTPUT_DIR, \"monte_carlo_long.csv\"), index=False)\n",
    "    mc_short.to_csv(os.path.join(OUTPUT_DIR, \"monte_carlo_longshort.csv\"), index=False)\n",
    "    plot_monte_carlo_scatter(mc_long, mc_short, name=\"monte_carlo_scatter.png\")\n",
    "\n",
    "    # Saving summary\n",
    "    summary_df = pd.DataFrame(results_summary)\n",
    "    summary_df.to_csv(os.path.join(OUTPUT_DIR, \"experiment_summary_full.csv\"), index=False)\n",
    "    print(\"Pipeline complete. Outputs saved to\", OUTPUT_DIR)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f2652-ca02-47eb-b2f0-0ce54db3a9a9",
   "metadata": {},
   "source": [
    "Below, when running as a standalone Python script, the model can be run by defining these factors:\n",
    "\n",
    "- Moving average windows of 20 and 40 days\n",
    "- Threshold of 2 bullish commodities\n",
    "- Minimum hold of 3 days\n",
    "- Realistic trading costs\n",
    "- A 3000-sample Monte Carlo run\n",
    "\n",
    "I am using this structure to allow me to run it both as an importable module and a self contained executable script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa036bfa-c2bf-4fd6-9ec8-5745ade476b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Corn (ZC=F)...\n",
      "Downloading Soybeans (ZS=F)...\n",
      "Downloading Cotton (CT=F)...\n",
      "Downloading Wheat (ZW=F)...\n",
      "Downloading DE (DE)...\n",
      "Downloading CAT (CAT)...\n",
      "Downloading AGCO (AGCO)...\n",
      "Downloading CNH (CNH)...\n",
      "Downloading 20+ Year U.S. Treasuries (TLT)...\n",
      "Downloading 7–10 Year Treasuries (IEF)...\n",
      "Downloading Gold (GLD)...\n",
      "Downloading Broad Commodities (energy-weighted) (DBC)...\n",
      "Downloading Developed Markets ex-US (EFA)...\n",
      "Downloading Vanguard Developed ex-US (VEA)...\n",
      "Downloading Anti-Beta (long low beta, short high beta) (BTAL)...\n",
      "Downloading Multi-strategy, alt-risk premia (LALT)...\n",
      "Downloading SPY (SPY)...\n",
      "Downloading QQQ (QQQ)...\n",
      "Downloading BND (BND)...\n",
      "Aligned rows: 6507 (2000-01-03 to 2025-11-18)\n",
      "Pipeline complete. Outputs saved to output\n",
      "   Ann_Return   Ann_Vol    Sharpe  Avg_Turnover  Total_Trades  Days_In_Market  \\\n",
      "0    0.054190  0.086166  0.612520      0.060012           781          4227.0   \n",
      "1    0.046002  0.082932  0.542363      0.044798           583          4157.0   \n",
      "\n",
      "   Final_Net_Equity  MA  MinConsec  MinHold  Commission  Slippage  \n",
      "0          3.549664  20          0        3      0.0005    0.0005  \n",
      "1          2.922961  40          0        3      0.0005    0.0005  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example run — tweak parameters as desired\n",
    "    summary = run_full_pipeline(\n",
    "        start=START,\n",
    "        ma_windows=[20, 40],\n",
    "        threshold=2,\n",
    "        min_consec_days=0,\n",
    "        min_hold_days=3,   # small minimum hold to reduce whipsaw\n",
    "        commission=0.0005,\n",
    "        slippage=0.0005,\n",
    "        n_mc=3000\n",
    "    )\n",
    "    print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_macos)",
   "language": "python",
   "name": "tf_macos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
